<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Parts2Whole generates realistic human images in various postures from images of body parts of any quatity or different origins.">
  <meta name="keywords" content="Parts2Whole, Human Image Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>From Parts to Whole: A Unified Reference Framework for Controllable Human Image Generation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<style>
  .container {
    max-width: 1200px;
    margin: 0 auto;
  }
</style>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/huanngzh">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://huanngzh.github.io/EpiDiff/">
            EpiDiff
          </a>
          <a class="navbar-item" href="https://huanngzh.github.io/Parts2Whole/">
            Parts2Whole
          </a>
          <!-- <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a> -->
        </div>
      </div>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">From Parts to Whole: A Unified Reference Framework for Controllable Human Image Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/huanngzh">Zehuan Huang</a><sup>1,2*</sup>,</span>
            <span class="author-block">
              <a href="">Hongxing Fan</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="">Lipeng Wang</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=_8lB7xcAAAAJ">Lu Sheng</a><sup>â€ </sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Beihang University,</span>
            <span class="author-block"><sup>2</sup>VAST</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2404.15267"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2404.15267"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/huanngzh/Parts2Whole"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/huanngzh/DeepFashion-Refs2Target"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              <!-- Demo Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-smile"></i>
                  </span>
                  <span>Demo (Coming soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.png" alt="Teaser Image" height="100%">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Parts2Whole</span> generates realistic human images in various postures from referential human part images<br> of any quantity and different origins.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in controllable human image generation have led to zero-shot generation using structural signals (e.g., pose, depth) or facial appearance. Yet, generating human images conditioned on multiple parts of human appearance remains challenging. Addressing this, we introduce Parts2Whole, a novel framework designed for generating customized portraits from multiple reference images, including pose images and various aspects of human appearance. To achieve this, we first develop a semantic-aware appearance encoder to retain details of different human parts, which processes each image based on its textual label to a series of multi-scale feature maps rather than one image token, preserving the image dimension. Second, our framework supports multi-image conditioned generation through a shared self-attention mechanism that operates across reference and target features during the diffusion process. We enhance the vanilla attention mechanism by incorporating mask information from the reference human images, allowing for precise selection of any part. Extensive experiments demonstrate the superiority of our approach over existing alternatives, offering advanced capabilities for multi-part controllable human image customization.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div>
      <div class="is-centered has-text-centered">
        <h2 class="title is-3">Method</h2>
      </div>
      <div class="hero-body" style="padding-bottom: 0;">
        <img src="./static/images/overview.png" alt="Method Overview" height="100%">
      </div>
      <div class="columns is-centered has-text-centered" style="padding-bottom: 3rem">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
              Overview of Parts2Whole. Based on the text-to-image diffusion model, our method designs an appearance encoder for encoding various parts of human appearance into multi-scale feature maps. We build this encoder by copying the network structure and pretrained weights from denoising U-Net. Features obtained from reference images with their textual labels are injected into the generation process by shared attention mechanism layer by layer. To precisely select the specified parts from reference images, we enhance the vanilla self-attention mechanism by incorporating subject masks in the reference images. Illustration of one block in U-Net is shown on the right part.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>

<section class="">
  <div class="container is-max-desktop">
    <!-- Results 1. -->
    <div>
      <div class="is-centered has-text-centered">
        <h2 class="title is-3">Customize Your Whole Body</h2>
      </div>
      <div class="hero-body" style="padding-bottom: 0;">
        <img src="./static/images/any_person.png" alt="Result 1" height="100%">
      </div>
      <div class="columns is-centered has-text-centered" style="padding-bottom: 3rem">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
            Parts2Whole supports generating human images conditioned on selected parts from different humans as control conditions. For example, the face from person A, the hair or headwear from person B, the upper clothes from person C, and the lower clothes from person D.
          </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Results 1. -->

    <!-- Results 2. -->
    <div>
      <div class="is-centered has-text-centered">
        <h2 class="title is-3">Specifiy Any Part</h2>
      </div>
      <div class="hero-body" style="padding-bottom: 0;">
        <img src="./static/images/page_any_quantity.png" alt="Result 2" height="100%">
      </div>
      <div class="columns is-centered has-text-centered" style="padding-bottom: 1.5rem">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <p>
            Parts2Whole supports generating human images from varying numbers of condition images, such as single hair or face input, or arbitrary combinations like "Face + Hair", "Face + Clothes", and "Upper body clothes + Lower body clothes".
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Results 2. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="has-text-centered">
      <h2 class="title">BibTeX</h2>
    </div>
    <div class="columns is-centered has-text-centered" style="padding-bottom: 1.5rem">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
<pre><code>@misc{huang2024parts2whole,
  title={From Parts to Whole: A Unified Reference Framework for Controllable Human Image Generation},
  author={Huang, Zehuan and Fan, Hongxing and Wang, Lipeng and Sheng, Lu},
  journal={arXiv preprint arXiv:2404.15267},
  year={2024}
}</code></pre>
        </div>
      </div>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/todo">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/huanngzh" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align: center;">
            The website template is borrowed from <a href="https://nerfies.github.io/" target="_blank">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
